{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b9c48-e5a0-422c-8f6b-82e971eee047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2: Define the MLP class\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights randomly\n",
    "        self.WH = np.random.rand(hidden_size, input_size) - 0.5  # Weights for input to hidden layer\n",
    "        self.WO = np.random.rand(output_size, hidden_size) - 0.5  # Weights for hidden to output layer\n",
    "        \n",
    "        # Initialize outputs\n",
    "        self.output_hidden = np.zeros(hidden_size)  # Hidden layer outputs\n",
    "        self.output_neuron = np.zeros(output_size)  # Output layer outputs\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Compute the sigmoid activation function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def feedforward(self, data_sample):\n",
    "        \"\"\"Perform the feedforward operation.\"\"\"\n",
    "        # Compute hidden layer outputs\n",
    "        for i in range(len(self.output_hidden)):\n",
    "            weighted_sum = np.dot(self.WH[i], data_sample)\n",
    "            self.output_hidden[i] = self.sigmoid(weighted_sum)\n",
    "        \n",
    "        # Compute output layer values\n",
    "        for i in range(len(self.output_neuron)):\n",
    "            weighted_sum = np.dot(self.WO[i], self.output_hidden)\n",
    "            self.output_neuron[i] = 1 if weighted_sum >= 0 else 0\n",
    "\n",
    "    def test_error(self, target_map):\n",
    "        \"\"\"Check if there is an error in the output.\"\"\"\n",
    "        for i in range(len(self.output_neuron)):\n",
    "            if target_map[i] != self.output_neuron[i]:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def train(self, data_sample, target_map, learning_rate=0.01):\n",
    "        \"\"\"Update weights based on the error.\"\"\"\n",
    "        # Calculate output error\n",
    "        output_error = target_map - self.output_neuron\n",
    "        \n",
    "        # Update weights between hidden and output layers\n",
    "        for i in range(len(self.WO)):\n",
    "            self.WO[i] += learning_rate * output_error[i] * self.output_hidden\n",
    "        \n",
    "        # Calculate hidden layer errors\n",
    "        hidden_error = np.dot(self.WO.T, output_error) * self.output_hidden * (1 - self.output_hidden)\n",
    "        \n",
    "        # Update weights between input and hidden layers\n",
    "        for i in range(len(self.WH)):\n",
    "            self.WH[i] += learning_rate * hidden_error[i] * data_sample\n",
    "\n",
    "# Step 3: Load dataset\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"Load the dataset and return input data and target labels.\"\"\"\n",
    "    dataset = pd.read_csv(file_path, header=None)\n",
    "    data = dataset.iloc[:, :-1].values  # First 64 columns are input\n",
    "    labels = dataset.iloc[:, -1].values  # Last column is the output\n",
    "    return data, labels\n",
    "\n",
    "# Step 4: Map target output\n",
    "def create_target_map(target_output, output_size=10):\n",
    "    \"\"\"Create a one-hot encoded map for the target output.\"\"\"\n",
    "    target_map = np.zeros(output_size)\n",
    "    target_map[target_output] = 1\n",
    "    return target_map\n",
    "\n",
    "# Step 5: Train and test the model\n",
    "def train_and_test_mlp(data1, labels1, data2, labels2, hidden_size=15, iterations=10):\n",
    "    \"\"\"Train the MLP and test its performance.\"\"\"\n",
    "    input_size = data1.shape[1]\n",
    "    output_size = 10\n",
    "    mlp = MLP(input_size, hidden_size, output_size)\n",
    "\n",
    "    for cycle in range(iterations):\n",
    "        success = 0\n",
    "\n",
    "        # Training phase\n",
    "        for i in range(len(data1)):\n",
    "            data_sample = data1[i]\n",
    "            target_output = labels1[i]\n",
    "            target_map = create_target_map(target_output)\n",
    "\n",
    "            mlp.feedforward(data_sample)\n",
    "            if mlp.test_error(target_map):\n",
    "                mlp.train(data_sample, target_map)\n",
    "            else:\n",
    "                success += 1\n",
    "\n",
    "        accuracy = success / len(data1)\n",
    "        print(f\"Cycle {cycle + 1}: Training Accuracy = {accuracy:.2%}\")\n",
    "\n",
    "    # Testing phase\n",
    "    success = 0\n",
    "    for i in range(len(data2)):\n",
    "        data_sample = data2[i]\n",
    "        target_output = labels2[i]\n",
    "        target_map = create_target_map(target_output)\n",
    "\n",
    "        mlp.feedforward(data_sample)\n",
    "        if not mlp.test_error(target_map):\n",
    "            success += 1\n",
    "\n",
    "    test_accuracy = success / len(data2)\n",
    "    print(f\"Test Accuracy = {test_accuracy:.2%}\")\n",
    "\n",
    "# Step 6: Add placeholders for dataset paths\n",
    "file1_path = \"cw2DataSet1.csv\"  # Placeholder for training data\n",
    "file2_path = \"cw2DataSet2.csv\"  # Placeholder for testing data\n",
    "\n",
    "# (Uncomment below when datasets are available on user's system)\n",
    "# data1, labels1 = load_dataset(file1_path)\n",
    "# data2, labels2 = load_dataset(file2_path)\n",
    "# train_and_test_mlp(data1, labels1, data2, labels2)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
